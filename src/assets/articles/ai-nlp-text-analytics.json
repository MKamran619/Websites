{
  "id": "ai-nlp-text-analytics",
  "title": "NLP and Text Analytics for Business Intelligence",
  "excerpt": "Extract insights from text data using sentiment analysis, entity recognition, and topic modeling for customer feedback, support tickets, and documents.",
  "date": "Jul 2023",
  "category": "AI & Machine Learning",
  "readTime": 12,
  "icon": "ðŸ¤–",
  "tags": ["NLP", "Text Analytics", "Sentiment", "Python"],
  "featured": false,
  "author": {
    "name": "Kamran Sohail",
    "role": "Software Engineer & Consultant",
    "avatar": "KS"
  },
  "content": "<h2>The Value of Text Analytics</h2><p>Unstructured text contains valuable insights. Customer reviews, support tickets, social media, and documents hold information that can drive business decisions when properly analyzed.</p><h2>Sentiment Analysis</h2><h3>Using Transformers</h3><pre><code>from transformers import pipeline\n\nsentiment_analyzer = pipeline(\n    \"sentiment-analysis\",\n    model=\"distilbert-base-uncased-finetuned-sst-2-english\"\n)\n\ndef analyze_sentiment(texts):\n    results = sentiment_analyzer(texts)\n    return [\n        {\n            'text': text,\n            'sentiment': r['label'],\n            'confidence': r['score']\n        }\n        for text, r in zip(texts, results)\n    ]\n\n# Analyze customer reviews\nreviews = [\n    \"Great product, works perfectly!\",\n    \"Terrible experience, never buying again.\"\n]\nsentiments = analyze_sentiment(reviews)</code></pre><h3>Fine-Tuning for Domain</h3><pre><code>from transformers import Trainer, TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=3,\n    per_device_train_batch_size=16,\n    evaluation_strategy='epoch'\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset\n)\n\ntrainer.train()</code></pre><h2>Named Entity Recognition</h2><pre><code>import spacy\n\nnlp = spacy.load('en_core_web_trf')\n\ndef extract_entities(text):\n    doc = nlp(text)\n    return [\n        {\n            'text': ent.text,\n            'label': ent.label_,\n            'start': ent.start_char,\n            'end': ent.end_char\n        }\n        for ent in doc.ents\n    ]\n\n# Extract from business document\ntext = \"Apple Inc. reported $394 billion in revenue in 2022. Tim Cook announced new products in California.\"\nentities = extract_entities(text)\n# Returns: Apple Inc. (ORG), $394 billion (MONEY), 2022 (DATE), Tim Cook (PERSON), California (GPE)</code></pre><h2>Topic Modeling</h2><pre><code>from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\n\n# Prepare documents\nvectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\ndoc_term_matrix = vectorizer.fit_transform(documents)\n\n# Train LDA model\nlda = LatentDirichletAllocation(\n    n_components=10,\n    random_state=42\n)\nlda.fit(doc_term_matrix)\n\n# Get topic words\ndef get_topic_words(model, vectorizer, n_words=10):\n    words = vectorizer.get_feature_names_out()\n    topics = []\n    for topic_idx, topic in enumerate(model.components_):\n        top_words = [words[i] for i in topic.argsort()[:-n_words-1:-1]]\n        topics.append(top_words)\n    return topics</code></pre><h2>Text Classification</h2><pre><code>from transformers import AutoTokenizer, AutoModelForSequenceClassification\n\n# Multi-label classification for support tickets\nclass TicketClassifier:\n    def __init__(self):\n        self.tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n        self.model = AutoModelForSequenceClassification.from_pretrained(\n            'bert-base-uncased',\n            num_labels=len(categories)\n        )\n        \n    def classify(self, ticket_text):\n        inputs = self.tokenizer(\n            ticket_text,\n            return_tensors='pt',\n            truncation=True\n        )\n        outputs = self.model(**inputs)\n        probs = torch.softmax(outputs.logits, dim=-1)\n        return {\n            'category': categories[probs.argmax()],\n            'confidence': probs.max().item()\n        }</code></pre><h2>Building a Text Analytics Pipeline</h2><pre><code>class TextAnalyticsPipeline:\n    def __init__(self):\n        self.sentiment = pipeline('sentiment-analysis')\n        self.ner = spacy.load('en_core_web_sm')\n        \n    def analyze(self, text):\n        return {\n            'sentiment': self.sentiment(text)[0],\n            'entities': self.extract_entities(text),\n            'summary': self.summarize(text),\n            'keywords': self.extract_keywords(text)\n        }</code></pre><h2>Production Tips</h2><ul><li>Pre-process text: lowercase, remove noise</li><li>Handle multiple languages with multilingual models</li><li>Cache model inference for repeated texts</li><li>Monitor for data drift over time</li><li>Use batch processing for high volume</li></ul><h2>Conclusion</h2><p>Modern NLP has made text analytics accessible. Start with pre-trained models and fine-tune for your specific domain and use cases.</p>"
}
