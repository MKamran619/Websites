{
  "id": "ai-llm-integration",
  "title": "Integrating Large Language Models into Enterprise Applications",
  "excerpt": "Practical guide to integrating LLMs like GPT-4 and Claude into production systems with best practices for prompting, context management, and cost optimization.",
  "date": "Jan 2024",
  "category": "AI & Machine Learning",
  "readTime": 14,
  "icon": "ðŸ¤–",
  "tags": ["LLM", "GPT", "AI", "Enterprise"],
  "featured": false,
  "author": {
    "name": "Kamran Sohail",
    "role": "Software Engineer & Consultant",
    "avatar": "KS"
  },
  "content": "<h2>The LLM Revolution</h2><p>Large Language Models have transformed what's possible in software applications. From customer support to code generation, LLMs are becoming essential enterprise tools.</p><h2>Integration Patterns</h2><h3>Direct API Integration</h3><pre><code>import OpenAI from 'openai';\n\nconst openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });\n\nasync function generateResponse(userMessage) {\n  const response = await openai.chat.completions.create({\n    model: 'gpt-4',\n    messages: [\n      { role: 'system', content: 'You are a helpful assistant.' },\n      { role: 'user', content: userMessage }\n    ],\n    temperature: 0.7,\n    max_tokens: 500\n  });\n  \n  return response.choices[0].message.content;\n}</code></pre><h3>RAG (Retrieval Augmented Generation)</h3><pre><code>async function answerWithContext(question) {\n  // 1. Convert question to embedding\n  const embedding = await openai.embeddings.create({\n    model: 'text-embedding-3-small',\n    input: question\n  });\n  \n  // 2. Search vector database for relevant documents\n  const relevantDocs = await vectorDb.search(embedding.data[0].embedding, 5);\n  \n  // 3. Generate answer with context\n  const context = relevantDocs.map(d => d.content).join('\\n\\n');\n  \n  return await openai.chat.completions.create({\n    model: 'gpt-4',\n    messages: [\n      { role: 'system', content: `Answer based on this context:\\n${context}` },\n      { role: 'user', content: question }\n    ]\n  });\n}</code></pre><h2>Prompt Engineering</h2><h3>System Prompts</h3><pre><code>const systemPrompt = `\nYou are a customer support agent for TechCorp.\n\nRules:\n- Be helpful and professional\n- Only answer questions about our products\n- If you don't know, say so - don't make up information\n- For billing issues, direct to billing@techcorp.com\n\nProduct catalog: [product details here]\n`;</code></pre><h3>Few-Shot Examples</h3><pre><code>const messages = [\n  { role: 'system', content: 'Extract order information from customer messages.' },\n  { role: 'user', content: 'I ordered 5 blue widgets yesterday' },\n  { role: 'assistant', content: '{\"product\": \"widget\", \"color\": \"blue\", \"quantity\": 5}' },\n  { role: 'user', content: 'Need 10 red gadgets shipped to NYC' },\n  { role: 'assistant', content: '{\"product\": \"gadget\", \"color\": \"red\", \"quantity\": 10, \"destination\": \"NYC\"}' },\n  { role: 'user', content: actualUserMessage }\n];</code></pre><h2>Production Considerations</h2><h3>Rate Limiting</h3><pre><code>const rateLimiter = new RateLimiter({\n  tokensPerInterval: 60,\n  interval: 'minute'\n});\n\nasync function limitedLLMCall(prompt) {\n  await rateLimiter.removeTokens(1);\n  return await openai.chat.completions.create(...);\n}</code></pre><h3>Caching</h3><pre><code>async function cachedLLMCall(prompt) {\n  const cacheKey = hash(prompt);\n  const cached = await redis.get(cacheKey);\n  if (cached) return JSON.parse(cached);\n  \n  const response = await openai.chat.completions.create(...);\n  await redis.setEx(cacheKey, 3600, JSON.stringify(response));\n  return response;\n}</code></pre><h3>Fallback Strategy</h3><pre><code>async function resilientLLMCall(prompt) {\n  try {\n    return await openai.chat.completions.create({ model: 'gpt-4', ... });\n  } catch (error) {\n    console.warn('GPT-4 failed, falling back to GPT-3.5');\n    return await openai.chat.completions.create({ model: 'gpt-3.5-turbo', ... });\n  }\n}</code></pre><h2>Cost Optimization</h2><ul><li>Use smaller models for simple tasks</li><li>Implement caching for repeated queries</li><li>Set appropriate max_tokens limits</li><li>Batch requests where possible</li><li>Monitor usage with alerts</li></ul><h2>Conclusion</h2><p>LLMs are powerful but require careful integration. Focus on reliability, cost management, and appropriate guardrails for production deployments.</p>"
}
