{
  "id": "cloud-kubernetes-production",
  "title": "Running Kubernetes in Production: Lessons Learned",
  "excerpt": "Real-world lessons from running Kubernetes clusters in production, covering cluster management, observability, and operational best practices.",
  "date": "Nov 2023",
  "category": "Cloud Computing",
  "readTime": 15,
  "icon": "☁️",
  "tags": ["Kubernetes", "Production", "DevOps", "Containers"],
  "featured": false,
  "author": {
    "name": "Kamran Sohail",
    "role": "Software Engineer & Consultant",
    "avatar": "KS"
  },
  "content": "<h2>Introduction</h2><p>Kubernetes has become the de facto standard for container orchestration. However, running it in production requires careful planning and operational maturity.</p><h2>Cluster Architecture</h2><h3>Control Plane High Availability</h3><ul><li>Run 3+ control plane nodes across availability zones</li><li>Use managed Kubernetes (EKS, AKS, GKE) when possible</li><li>Separate etcd to dedicated nodes for large clusters</li></ul><h3>Node Pools</h3><pre><code>Node Pools:\n├── system-pool (m5.large)\n│   └── Core system components\n├── general-pool (m5.xlarge)\n│   └── Standard workloads\n├── memory-pool (r5.2xlarge)\n│   └── Memory-intensive apps\n└── gpu-pool (p3.2xlarge)\n    └── ML workloads</code></pre><h2>Resource Management</h2><h3>Requests and Limits</h3><p>Always set both requests and limits:</p><pre><code>resources:\n  requests:\n    memory: \"256Mi\"\n    cpu: \"250m\"\n  limits:\n    memory: \"512Mi\"\n    cpu: \"500m\"</code></pre><h3>Pod Disruption Budgets</h3><pre><code>apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: api-pdb\nspec:\n  minAvailable: 2\n  selector:\n    matchLabels:\n      app: api</code></pre><h3>Priority Classes</h3><p>Ensure critical workloads survive resource pressure:</p><pre><code>apiVersion: scheduling.k8s.io/v1\nkind: PriorityClass\nmetadata:\n  name: high-priority\nvalue: 1000000\nglobalDefault: false</code></pre><h2>Networking</h2><h3>Network Policies</h3><p>Implement zero-trust networking:</p><pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: api-policy\nspec:\n  podSelector:\n    matchLabels:\n      app: api\n  ingress:\n  - from:\n    - podSelector:\n        matchLabels:\n          app: frontend\n    ports:\n    - port: 8080</code></pre><h2>Observability</h2><h3>The Three Pillars</h3><ul><li><strong>Metrics:</strong> Prometheus + Grafana</li><li><strong>Logs:</strong> Fluent Bit → Elasticsearch/Loki</li><li><strong>Traces:</strong> OpenTelemetry → Jaeger/Tempo</li></ul><h3>Essential Dashboards</h3><ul><li>Cluster resource utilization</li><li>Node health and capacity</li><li>Pod restart rates</li><li>API server latency</li><li>etcd performance</li></ul><h2>Security</h2><h3>Pod Security Standards</h3><pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: production\n  labels:\n    pod-security.kubernetes.io/enforce: restricted</code></pre><h3>Image Security</h3><ul><li>Scan images for vulnerabilities (Trivy, Clair)</li><li>Sign images (Cosign, Notary)</li><li>Use admission controllers to enforce policies</li></ul><h2>Disaster Recovery</h2><ul><li>Regular etcd backups</li><li>GitOps for configuration recovery</li><li>Multi-region deployment strategy</li><li>Tested recovery procedures</li></ul><h2>Lessons Learned</h2><ol><li>Start with managed Kubernetes</li><li>Invest in observability from day one</li><li>Automate everything with GitOps</li><li>Practice failure scenarios regularly</li><li>Keep clusters reasonably sized</li></ol><h2>Conclusion</h2><p>Production Kubernetes requires significant operational investment. Build your platform team's expertise gradually and automate relentlessly.</p>"
}
