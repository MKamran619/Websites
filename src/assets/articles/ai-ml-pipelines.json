{
  "id": "ai-ml-pipelines",
  "title": "Building Production ML Pipelines with MLflow",
  "excerpt": "Create end-to-end machine learning pipelines that handle data preparation, model training, versioning, and deployment with MLflow and Python.",
  "date": "Nov 2023",
  "category": "AI & Machine Learning",
  "readTime": 13,
  "icon": "ðŸ¤–",
  "tags": ["MLflow", "MLOps", "Pipeline", "Python"],
  "featured": false,
  "author": {
    "name": "Kamran Sohail",
    "role": "Software Engineer & Consultant",
    "avatar": "KS"
  },
  "content": "<h2>The MLOps Challenge</h2><p>Moving ML models from notebooks to production requires robust pipelines for training, versioning, and deployment. MLflow provides a comprehensive platform for this.</p><h2>MLflow Components</h2><ul><li><strong>Tracking:</strong> Log experiments, parameters, metrics</li><li><strong>Projects:</strong> Package ML code for reproducibility</li><li><strong>Models:</strong> Manage and deploy models</li><li><strong>Registry:</strong> Central model store with versioning</li></ul><h2>Experiment Tracking</h2><pre><code>import mlflow\nimport mlflow.sklearn\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, f1_score\n\nmlflow.set_tracking_uri(\"http://mlflow-server:5000\")\nmlflow.set_experiment(\"customer-churn\")\n\nwith mlflow.start_run():\n    # Log parameters\n    mlflow.log_param(\"n_estimators\", 100)\n    mlflow.log_param(\"max_depth\", 10)\n    \n    # Train model\n    model = RandomForestClassifier(n_estimators=100, max_depth=10)\n    model.fit(X_train, y_train)\n    \n    # Log metrics\n    predictions = model.predict(X_test)\n    mlflow.log_metric(\"accuracy\", accuracy_score(y_test, predictions))\n    mlflow.log_metric(\"f1_score\", f1_score(y_test, predictions))\n    \n    # Log model\n    mlflow.sklearn.log_model(model, \"model\")</code></pre><h2>Pipeline Definition</h2><pre><code># MLproject file\nname: customer-churn\n\nconda_env: conda.yaml\n\nentry_points:\n  preprocess:\n    parameters:\n      input_path: path\n    command: \"python preprocess.py --input {input_path}\"\n    \n  train:\n    parameters:\n      n_estimators: {type: int, default: 100}\n      max_depth: {type: int, default: 10}\n    command: \"python train.py --n_estimators {n_estimators} --max_depth {max_depth}\"\n    \n  evaluate:\n    parameters:\n      model_uri: str\n    command: \"python evaluate.py --model_uri {model_uri}\"</code></pre><h2>Model Registry</h2><pre><code># Register model\nresult = mlflow.register_model(\n    \"runs:/abc123/model\",\n    \"customer-churn-model\"\n)\n\n# Transition to production\nclient = mlflow.tracking.MlflowClient()\nclient.transition_model_version_stage(\n    name=\"customer-churn-model\",\n    version=1,\n    stage=\"Production\"\n)</code></pre><h2>Model Deployment</h2><h3>REST API Serving</h3><pre><code># Serve model as REST API\nmlflow models serve -m models:/customer-churn-model/Production -p 5001</code></pre><h3>Docker Deployment</h3><pre><code># Build Docker image\nmlflow models build-docker -m models:/customer-churn-model/Production -n churn-model\n\n# Run container\ndocker run -p 5001:8080 churn-model</code></pre><h3>Inference Code</h3><pre><code>import requests\nimport json\n\ndef predict(features):\n    response = requests.post(\n        \"http://localhost:5001/invocations\",\n        headers={\"Content-Type\": \"application/json\"},\n        data=json.dumps({\"inputs\": features})\n    )\n    return response.json()</code></pre><h2>Automated Retraining</h2><pre><code>from airflow import DAG\nfrom airflow.operators.python import PythonOperator\n\ndef retrain_model():\n    # Run MLflow project\n    mlflow.projects.run(\n        uri=\".\",\n        entry_point=\"train\",\n        parameters={\"n_estimators\": 100}\n    )\n\ndag = DAG('ml_retrain', schedule_interval='@weekly')\n\nretrain = PythonOperator(\n    task_id='retrain_model',\n    python_callable=retrain_model,\n    dag=dag\n)</code></pre><h2>Best Practices</h2><ul><li>Version everything: data, code, models</li><li>Automate testing for model quality</li><li>Monitor model drift in production</li><li>Implement gradual rollouts (canary deployments)</li><li>Document model cards for each version</li></ul><h2>Conclusion</h2><p>MLOps brings DevOps practices to machine learning. MLflow provides the foundation for reproducible, scalable ML pipelines in production.</p>"
}
